{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70e76ef0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyodbc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 56\u001b[0m\n\u001b[0;32m     48\u001b[0m REQUIRED_COLUMNS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSC_CODE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymbol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSC_GROUP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSC_TYPE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLast\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     49\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrev Close\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNO_TRADES\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNET_TURNOV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISIN_CODE\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     51\u001b[0m column_mappings \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpclose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrev Close\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvwap\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVWAP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrades\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrades\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdvolume\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeliverable Volume\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdeliverble\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mDeliverble\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m---> 56\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmssql+pyodbc://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mHOST\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDB\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m?trusted_connection=yes&driver=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDRIVER\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m realpath_bhavcopy \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), BHAVCOPY_FOLDER)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(realpath_bhavcopy):\n",
      "File \u001b[1;32m<string>:2\u001b[0m, in \u001b[0;36mcreate_engine\u001b[1;34m(url, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\PycharmProjects\\memgpt\\venv\\lib\\site-packages\\sqlalchemy\\util\\deprecations.py:281\u001b[0m, in \u001b[0;36mdeprecated_params.<locals>.decorate.<locals>.warned\u001b[1;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    275\u001b[0m         _warn_with_version(\n\u001b[0;32m    276\u001b[0m             messages[m],\n\u001b[0;32m    277\u001b[0m             versions[m],\n\u001b[0;32m    278\u001b[0m             version_warnings[m],\n\u001b[0;32m    279\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    280\u001b[0m         )\n\u001b[1;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\PycharmProjects\\memgpt\\venv\\lib\\site-packages\\sqlalchemy\\engine\\create.py:601\u001b[0m, in \u001b[0;36mcreate_engine\u001b[1;34m(url, **kwargs)\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    600\u001b[0m             dbapi_args[k] \u001b[38;5;241m=\u001b[39m pop_kwarg(k)\n\u001b[1;32m--> 601\u001b[0m     dbapi \u001b[38;5;241m=\u001b[39m dbapi_meth(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdbapi_args)\n\u001b[0;32m    603\u001b[0m dialect_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbapi\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dbapi\n\u001b[0;32m    605\u001b[0m dialect_args\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_linting\u001b[39m\u001b[38;5;124m\"\u001b[39m, compiler\u001b[38;5;241m.\u001b[39mNO_LINTING)\n",
      "File \u001b[1;32m~\\PycharmProjects\\memgpt\\venv\\lib\\site-packages\\sqlalchemy\\connectors\\pyodbc.py:60\u001b[0m, in \u001b[0;36mPyODBCConnector.import_dbapi\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimport_dbapi\u001b[39m(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModuleType:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyodbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyodbc'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text as sa_text\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# database configuration\n",
    "HOST = \"DESKTOP-0H6SQRG\"\n",
    "DB = \"nse\"\n",
    "DRIVER = \"ODBC+Driver+11+for+SQL+Server\"\n",
    "\n",
    "\n",
    "ERROR_TABLE_NAME = \"bse_error_dates\"\n",
    "HISTORY_TABLE = \"bse_history\"\n",
    "BHAVCOPY_FOLDER = \"bse_bhavcopies\"\n",
    "\n",
    "CREATE_ERROR_TABLE_QUERY = f\"\"\"IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='{ERROR_TABLE_NAME}' and xtype='U')\n",
    "                                create table {ERROR_TABLE_NAME}(id int primary key identity(1,1), [date] date not null,\n",
    "                                timestamp datetime default current_timestamp)\"\"\"\n",
    "\n",
    "CREATE_BSE_HISTORY_TABLE_QUERY = f\"\"\"IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='{HISTORY_TABLE}' and xtype='U')\n",
    "                                CREATE TABLE {HISTORY_TABLE}(\n",
    "                                [id] [bigint] primary key identity(1,1),\n",
    "                                [Date] [date],\n",
    "                                [Symbol] [varchar](max),\n",
    "                                [Open] [float] NULL,\n",
    "                                [High] [float] NULL,\n",
    "                                [Low] [float] NULL,\n",
    "                                [Close] [float] NULL,\n",
    "                                [Last] [float] NULL,\n",
    "                                [Prev Close] [float] NULL,\n",
    "                                [Volume] [bigint] NULL,\n",
    "                                [NO_TRADES] [bigint] NULL,\n",
    "                                [SC_CODE] [bigint],\n",
    "                                [SC_GROUP] [varchar](max),\n",
    "                                [SC_TYPE] [varchar](max),\n",
    "                                [NET_TURNOV] [float] NULL,\n",
    "                                [ISIN_CODE] [varchar](max) NULL,\n",
    "                                timestamp datetime default current_timestamp)\"\"\"\n",
    "\n",
    "REQUIRED_COLUMNS = ['Date', 'SC_CODE', 'Symbol', 'SC_GROUP', 'SC_TYPE', 'Open', 'High', 'Low', 'Close', 'Last', \n",
    "                    'Prev Close', 'NO_TRADES', 'Volume', 'NET_TURNOV', 'ISIN_CODE']\n",
    "\n",
    "column_mappings = {\n",
    "    \"date\": \"Date\", \"symbol\": \"Symbol\", \"pclose\": \"Prev Close\", \"open\": \"Open\", \"high\": \"High\", \"low\":\"Low\", \"close\": \"Close\",\n",
    "    \"vwap\": \"VWAP\", \"volume\": \"Volume\", \"trades\": \"Trades\", \"dvolume\": \"Deliverable Volume\", \"pdeliverble\":\"%Deliverble\",\n",
    "    \"series\": \"Series\", \"last\": \"Last\"}\n",
    "\n",
    "engine = create_engine(f'mssql+pyodbc://{HOST}/{DB}?trusted_connection=yes&driver={DRIVER}')\n",
    "realpath_bhavcopy = os.path.join(os.getcwd(), BHAVCOPY_FOLDER)\n",
    "if not os.path.exists(realpath_bhavcopy):\n",
    "    os.mkdir(realpath_bhavcopy)\n",
    "\n",
    "def execute_sql(query, commit=False):\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query)\n",
    "        if commit:\n",
    "            conn.commit()\n",
    "    return result\n",
    "\n",
    "    \n",
    "execute_sql(CREATE_ERROR_TABLE_QUERY, commit=True)\n",
    "execute_sql(CREATE_BSE_HISTORY_TABLE_QUERY, commit=True)\n",
    "\n",
    "\n",
    "def convert_strto_datetime(date_time):\n",
    "    try:\n",
    "        datetime_str = datetime.strptime(date_time, '%d-%b-%Y')\n",
    "    except ValueError:\n",
    "        datetime_str = datetime.strptime(date_time, '%d-%m-%Y')\n",
    "    return datetime_str\n",
    "\n",
    "def if_exists(tablename, date=None, df=None):\n",
    "    DUPLICATE_CHECK_QUERY = \"\"\"select * from {TABLENAME} where {DATE_COL}='{DATE}' \n",
    "                                and {SYMBOL_COL}='{SYMBOL}'\"\"\"\n",
    "    SINGLE_CHECK_QUERY = \"select * from {TABLENAME} where {DATE_COL}='{DATE}'\"\n",
    "    result = None\n",
    "    if date:\n",
    "        result = execute_sql(SINGLE_CHECK_QUERY.format(TABLENAME=tablename, DATE_COL=column_mappings[\"date\"],\n",
    "                                                           DATE=date)).fetchone()\n",
    "        \n",
    "    elif df is not None:\n",
    "        rand_rec = df.sample()\n",
    "        rand_rec = rand_rec.to_dict(\"records\")[0]\n",
    "        result = execute_sql(DUPLICATE_CHECK_QUERY.format(TABLENAME=tablename, \n",
    "                                                           DATE_COL=column_mappings[\"date\"],\n",
    "                                                           SYMBOL_COL=column_mappings[\"symbol\"], \n",
    "                                                           DATE=rand_rec[column_mappings[\"date\"]],\n",
    "                                                           SYMBOL=rand_rec[column_mappings[\"symbol\"]])).fetchone()\n",
    "    if result:\n",
    "        return True\n",
    "    return False\n",
    "        \n",
    "\n",
    "def download(download_url, fpath):\n",
    "    headers = {\n",
    "      'authority': 'www.bseindia.com',\n",
    "      'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "      'accept-language': 'en-US,en;q=0.9',\n",
    "      'referer': 'https://www.bseindia.com/markets/marketinfo/BhavCopy.aspx',\n",
    "      'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\n",
    "                      (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36 Edg/106.0.1370.42'\n",
    "    }\n",
    "    with open(fpath, \"wb\") as fp:\n",
    "        r = requests.get(download_url, stream=True, verify=False, headers=headers)\n",
    "        if r.ok:\n",
    "            fp.write(r.content)\n",
    "    return True\n",
    "\n",
    "def full_bhavcopy_bse(date, folder):\n",
    "    filename = f'EQ_ISINCODE_{\"%02d\"%date.day}{\"%02d\"%date.month}{date.strftime(\"%y\")}.ZIP' \n",
    "    file_path = os.path.join(folder, filename)\n",
    "    csv_path = file_path.replace(\".ZIP\", \".csv\")\n",
    "    download_url = f'https://www.bseindia.com/download/BhavCopy/Equity/{filename}'\n",
    "    if not os.path.exists(csv_path):\n",
    "        if download(download_url, file_path):\n",
    "            try:\n",
    "                with zipfile.ZipFile(file_path, \"r\") as compressed_file:\n",
    "                    compressed_file.extractall(Path(file_path).parent)\n",
    "#                 print(f\"downloaded data for {date}\")\n",
    "                os.remove(file_path)\n",
    "                return csv_path\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        return None\n",
    "    return csv_path\n",
    "\n",
    "def clean_df(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df.rename(columns={\"SC_NAME\": column_mappings[\"symbol\"], \"OPEN\": column_mappings[\"open\"], \"HIGH\":column_mappings[\"high\"], \n",
    "         \"LOW\":column_mappings[\"low\"], \"CLOSE\":column_mappings[\"close\"], \"NO_OF_SHRS\":column_mappings[\"volume\"],\n",
    "        \"PREVCLOSE\":column_mappings[\"pclose\"], \"TRADING_DATE\":column_mappings[\"date\"], \"LAST\": column_mappings[\"last\"]})\n",
    "    return df[REQUIRED_COLUMNS]\n",
    "\n",
    "def save_bc(date):\n",
    "    filepath = full_bhavcopy_bse(date, BHAVCOPY_FOLDER)\n",
    "    df = clean_df(filepath)\n",
    "    df.to_sql(HISTORY_TABLE, engine, index=False, if_exists=\"append\")\n",
    "    print(f\"Downloaded data for {date.strftime('%d-%b-%Y')}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19125f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36b3754b",
   "metadata": {},
   "source": [
    "## download data from existing table based on last date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5b703f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last date found in database is 12-May-2023\n",
      "Downloaded data for 15-May-2023\n",
      "Downloaded data for 16-May-2023\n",
      "Downloaded data for 17-May-2023\n",
      "Downloaded data for 18-May-2023\n",
      "Downloaded data for 19-May-2023\n",
      "Downloaded data for 22-May-2023\n",
      "Downloaded data for 23-May-2023\n",
      "Downloaded data for 24-May-2023\n",
      "Downloaded data for 25-May-2023\n",
      "Downloaded data for 26-May-2023\n",
      "Downloaded data for 29-May-2023\n",
      "Downloaded data for 30-May-2023\n",
      "Downloaded data for 31-May-2023\n",
      "Downloaded data for 01-Jun-2023\n",
      "Downloaded data for 02-Jun-2023\n",
      "Downloaded data for 05-Jun-2023\n",
      "Downloaded data for 06-Jun-2023\n",
      "Downloaded data for 07-Jun-2023\n",
      "Downloaded data for 08-Jun-2023\n",
      "Downloaded data for 09-Jun-2023\n",
      "Downloaded data for 12-Jun-2023\n",
      "Downloaded data for 13-Jun-2023\n",
      "Downloaded data for 14-Jun-2023\n",
      "Downloaded data for 15-Jun-2023\n",
      "Downloaded data for 16-Jun-2023\n",
      "Downloaded data for 19-Jun-2023\n",
      "Downloaded data for 20-Jun-2023\n",
      "Downloaded data for 21-Jun-2023\n",
      "Downloaded data for 22-Jun-2023\n",
      "Downloaded data for 23-Jun-2023\n",
      "Downloaded data for 26-Jun-2023\n",
      "Downloaded data for 27-Jun-2023\n",
      "Downloaded data for 28-Jun-2023\n",
      "File is not a zip file\n",
      "error occured on 29-Jun-2023\n",
      "Downloaded data for 30-Jun-2023\n",
      "Downloaded data for 03-Jul-2023\n",
      "Downloaded data for 04-Jul-2023\n",
      "Downloaded data for 05-Jul-2023\n",
      "Downloaded data for 06-Jul-2023\n",
      "Downloaded data for 07-Jul-2023\n",
      "Downloaded data for 10-Jul-2023\n",
      "Downloaded data for 11-Jul-2023\n",
      "Downloaded data for 12-Jul-2023\n",
      "Downloaded data for 13-Jul-2023\n",
      "Downloaded data for 14-Jul-2023\n",
      "Downloaded data for 17-Jul-2023\n",
      "Downloaded data for 18-Jul-2023\n",
      "Downloaded data for 19-Jul-2023\n",
      "Downloaded data for 20-Jul-2023\n",
      "Downloaded data for 21-Jul-2023\n",
      "Downloaded data for 24-Jul-2023\n",
      "Downloaded data for 25-Jul-2023\n",
      "Downloaded data for 26-Jul-2023\n",
      "Downloaded data for 27-Jul-2023\n",
      "Downloaded data for 28-Jul-2023\n",
      "Downloaded data for 31-Jul-2023\n",
      "Downloaded data for 01-Aug-2023\n",
      "Downloaded data for 02-Aug-2023\n",
      "Downloaded data for 03-Aug-2023\n",
      "Downloaded data for 04-Aug-2023\n",
      "Downloaded data for 07-Aug-2023\n",
      "Downloaded data for 08-Aug-2023\n",
      "Downloaded data for 09-Aug-2023\n",
      "Downloaded data for 10-Aug-2023\n",
      "Downloaded data for 11-Aug-2023\n",
      "Downloaded data for 14-Aug-2023\n",
      "File is not a zip file\n",
      "error occured on 15-Aug-2023\n",
      "Downloaded data for 16-Aug-2023\n",
      "Downloaded data for 17-Aug-2023\n",
      "Downloaded data for 18-Aug-2023\n",
      "Downloaded data for 21-Aug-2023\n",
      "Downloaded data for 22-Aug-2023\n",
      "Downloaded data for 23-Aug-2023\n",
      "Downloaded data for 24-Aug-2023\n",
      "Downloaded data for 25-Aug-2023\n",
      "Downloaded data for 28-Aug-2023\n",
      "Downloaded data for 29-Aug-2023\n",
      "Downloaded data for 30-Aug-2023\n",
      "Downloaded data for 31-Aug-2023\n",
      "Downloaded data for 01-Sep-2023\n",
      "Downloaded data for 04-Sep-2023\n",
      "Downloaded data for 05-Sep-2023\n",
      "Downloaded data for 06-Sep-2023\n",
      "Downloaded data for 07-Sep-2023\n",
      "Downloaded data for 08-Sep-2023\n",
      "Downloaded data for 11-Sep-2023\n",
      "Downloaded data for 12-Sep-2023\n",
      "Downloaded data for 13-Sep-2023\n",
      "Downloaded data for 14-Sep-2023\n",
      "File is not a zip file\n",
      "error occured on 15-Sep-2023\n"
     ]
    }
   ],
   "source": [
    "last_date = execute_sql(f\"select top(1){column_mappings['date']} from {HISTORY_TABLE} order by {column_mappings['date']} DESC\").fetchone()\n",
    "print(f\"last date found in database is {last_date[column_mappings['date']].strftime('%d-%b-%Y')}\")\n",
    "next_date = last_date[column_mappings['date']]\n",
    "yesterday = date.today()-timedelta(days=1)\n",
    "while True:\n",
    "    try:\n",
    "        if yesterday < next_date:\n",
    "            break\n",
    "        next_date = next_date+timedelta(days=1)\n",
    "        if next_date.weekday() < 5:\n",
    "    #         print(next_date.strftime('%d-%b-%Y'))\n",
    "            try:\n",
    "                if if_exists(HISTORY_TABLE, next_date):\n",
    "                    print(f\"Data already exists for {next_date.strftime('%d-%b-%Y')} in {HISTORY_TABLE} table\")\n",
    "                    continue\n",
    "                df = save_bc(next_date)\n",
    "            except Exception as e:\n",
    "                print(f\"error occured on {next_date.strftime('%d-%b-%Y')}\")\n",
    "                execute_sql(f\"insert into {ERROR_TABLE_NAME}([date]) values('{next_date}')\", commit=True)\n",
    "                continue\n",
    "        time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Download interrupted..\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae3e08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "645ed871",
   "metadata": {},
   "source": [
    "## Download data for the date in error table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f91198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "Error tokenizing data. C error: Expected 18 fields in line 1796, saw 32\n",
      "\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "Error tokenizing data. C error: Expected 18 fields in line 1821, saw 30\n",
      "\n",
      "Error tokenizing data. C error: Expected 18 fields in line 3183, saw 35\n",
      "\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n",
      "Downloaded data for 07-Nov-2022\n",
      "File is not a zip file\n",
      "Invalid file path or buffer object type: <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "err_dates = execute_sql(f\"select [date] as {column_mappings['date']} from {ERROR_TABLE_NAME}\").fetchall()\n",
    "for d in err_dates:\n",
    "    try:\n",
    "#         print(d[column_mappings[\"date\"]].strftime('%d-%b-%Y'))\n",
    "        save_bc(d[column_mappings[\"date\"]])\n",
    "        execute_sql(f\"delete {ERROR_TABLE_NAME} where date='{d[column_mappings['date']]}'\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    time.sleep(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34bc4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c0fc912",
   "metadata": {},
   "source": [
    "## Download data for a Single day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4d216ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data for 03-Jan-2022\n"
     ]
    }
   ],
   "source": [
    "download_date = \"03-01-2022\"                #dd/mm/yyyy\n",
    "\n",
    "dt = convert_strto_datetime(download_date)\n",
    "if dt.weekday() > 4:\n",
    "    print(f\"Sorry its a weekend.. {download_date}\")\n",
    "elif if_exists(HISTORY_TABLE, dt):\n",
    "    print(f\"Data already exists for {download_date} in {HISTORY_TABLE} table\")\n",
    "else:\n",
    "    df = save_bc(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db9fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0277b410-5e74-40fa-9f67-78cd989d0c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4aca88-db75-4f8b-8f57-c6057b2d6b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815b3a2-302f-4f24-a1d4-10216221d149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85bb7d-68ec-4900-b010-9700f4964e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
